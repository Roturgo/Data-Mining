{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFS 770 - Assignment 4\n",
    "\n",
    "**Note**: Created using Anaconda Python 3.7.3 (64-bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-task setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.models import LdaModel, LsiModel\n",
    "from pprint import pprint\n",
    "\n",
    "# Sci-Py Packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# nltk packages\n",
    "import nltk\n",
    "from nltk import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Read data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.1: Load data\n",
    "data_file = \"amazon_review_texts.csv\"\n",
    "df = pd.read_csv(data_file,\n",
    "                 sep=\",\",\n",
    "                 header='infer',\n",
    "                 )\n",
    "\n",
    "# Get a listing of our categories\n",
    "categories = list(df.columns)\n",
    "#pprint(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          pid helpful  score  \\\n",
      "0  B000GAYQL8     0/0      5   \n",
      "1  B000IBNPDA     0/0      5   \n",
      "2  B000J2HA16     0/0      5   \n",
      "3  B000BDIQPM     0/0      5   \n",
      "4  B000GZTH9E     0/3      4   \n",
      "\n",
      "                                                text category  \n",
      "0  GREAT WATCH AND GREAT LOOK. BIG FACE AND 4 DIF...    watch  \n",
      "1  Bought this as a Christmas gift, my boyfriend ...    watch  \n",
      "2  I love this watch! Its sporty, without looking...    watch  \n",
      "3  Works great,looks nice,dont have to worry abou...    watch  \n",
      "4  I need to change the watch wrist and I havent ...    watch  \n"
     ]
    }
   ],
   "source": [
    "# Q1.2: Make sure we've got data in our dataframe\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    2070\n",
       "4     773\n",
       "1     595\n",
       "3     303\n",
       "2     259\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1.3: Show the distribution counts of the variable \"score\"\n",
    "df['score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "automotive     1000\n",
       "electronics    1000\n",
       "watch          1000\n",
       "software       1000\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1.4: Show the distribution counts of the variable \"category\"\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Tokenize the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.1: Tokenize, performing the following:\n",
    "##      - lowercase\n",
    "##      - remove stopwords\n",
    "##      - perform stemming\n",
    "\n",
    "# get a set of stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "import re\n",
    "def before_token(documents):\n",
    "    # conver words to lower case\n",
    "    lower = map(str.lower, documents)\n",
    "    # remove puntuations\n",
    "    punctuationless = list(map(lambda x: \" \".join(re.findall('\\\\b\\\\w\\\\w+\\\\b',x)), lower))\n",
    "    # remove numbers\n",
    "    return list(map(lambda x:re.sub('\\\\b[0-9]+\\\\b', '', x), punctuationless))\n",
    "\n",
    "# initialize a stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# initialize a container of token frequencies\n",
    "fdist = nltk.FreqDist()\n",
    "\n",
    "\n",
    "# define a function that preprocess a single document and returns a list of tokens\n",
    "def preprocess(doc):\n",
    "    tokens = []\n",
    "    for token in doc.split():\n",
    "        if token not in stopwords:\n",
    "            tokens.append(stemmer.stem(token))\n",
    "    return tokens\n",
    "            \n",
    "# preprocess all documents\n",
    "processed = list(map(preprocess, before_token(df['text'])))\n",
    "\n",
    "\n",
    "#print(processed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens: 10973\n",
      "Total tokens: 193927\n",
      "Tokens occurred only once: 4701\n"
     ]
    }
   ],
   "source": [
    "# Q2.2: Calculate word frequency distribution\n",
    "#type(processed)\n",
    "\n",
    "# calculate the token frequency\n",
    "# the FreqDist function takes in a list of tokens and return a dict containg unique tokens and frequency\n",
    "fdist = nltk.FreqDist([token for doc in processed for token in doc])\n",
    "\n",
    "print(\"Unique tokens: %d\" % fdist.B())\n",
    "print(\"Total tokens: %d\" % fdist.N())\n",
    "print(\"Tokens occurred only once: %d\" % len(fdist.hapaxes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  watch     use     one    work    time    like product   great     get   would \n",
      "   2553    2476    1795    1605    1420    1375    1336    1318    1309    1217 \n"
     ]
    }
   ],
   "source": [
    "# Q2.3: Print top 10 most frequent words\n",
    "\n",
    "fdist.tabulate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Which of the top 10 words might not be useful in text clustering and classification, and why?**\n",
    "\n",
    "A: These words from the top 10 may not be useful in text clustering:\n",
    "* use\n",
    "* one\n",
    "* work\n",
    "* like\n",
    "* product\n",
    "* great\n",
    "* get \n",
    "* would\n",
    "\n",
    "These words could appear in the reviews for almost any of the product categories, so I don't think they'll be helpful in classifying our reviews into categories.  Many of them are stop words, and will be removed in later steps.\n",
    "\n",
    "The two words \"watch\" and \"time\", however, are quite unique and in fact one of them is specifically one of our categories.  The word \"time\" would be much more likely to be observed in reviews for the watch category, so I think it will be incredibly useful in classifying our review data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.1: Reconstruct the documents\n",
    "processed_doc = list(map(\" \".join, processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.2: Vectorize all of the documents, use norm=l2\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(norm=\"l2\",\n",
    "                             max_df=0.8, # remove frequent words (>80%)\n",
    "                             stop_words='english' # remove English stopwords stored in Scikit-Learn\n",
    "                            )\n",
    "\n",
    "X = vectorizer.fit_transform(processed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 4000, n_features: 10833\n"
     ]
    }
   ],
   "source": [
    "# Q3.3: Print the number of features extracted by the TFIDF vectorizer\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=4, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=54321, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4.1: Categorize the documents into 4 clusters\n",
    "km = KMeans(n_clusters=4, max_iter=100, random_state=54321)\n",
    "\n",
    "# Fit using our data\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " use\n",
      " product\n",
      " work\n",
      " great\n",
      " good\n",
      " instal\n",
      " program\n",
      " like\n",
      " softwar\n",
      " time\n",
      "Cluster 1:\n",
      " batteri\n",
      " charg\n",
      " charger\n",
      " power\n",
      " adapt\n",
      " appl\n",
      " camera\n",
      " canon\n",
      " work\n",
      " origin\n",
      "Cluster 2:\n",
      " watch\n",
      " look\n",
      " band\n",
      " time\n",
      " great\n",
      " wear\n",
      " love\n",
      " like\n",
      " nice\n",
      " price\n",
      "Cluster 3:\n",
      " bed\n",
      " air\n",
      " inflat\n",
      " comfort\n",
      " pump\n",
      " sleep\n",
      " mattress\n",
      " deflat\n",
      " airb\n",
      " easi\n"
     ]
    }
   ],
   "source": [
    "# Q4.2: Print the top 10 representative words for each cluster\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(4):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: How well you think these words describe the 4 product categories?**\n",
    "\n",
    "A: From the output of Q1.4, we know that our 4 categories are:\n",
    "1. software\n",
    "2. electronics\n",
    "3. automotive\n",
    "4. watch  \n",
    "\n",
    "For our K-means clusters, 3 out of the 4 categories match up very well to the clusters:\n",
    "\n",
    "* Cluster 0 maps to software\n",
    "* Cluster 1 maps to electronics\n",
    "* Cluster 2 maps to watch\n",
    "\n",
    "However, Cluster 3 does not seem to map up well with the remaining category of automotive.  It seems that Cluster 3 maps up to an inflatable mattress, not anything automotive.  I think that's a major problem in our K-means clustering results, and could send us down the wrong path for further analyzing that 4th category if we were to rely on the current clustering results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Build a topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5.1: Build a topic model using Latent Dirichlet Allocation (LDA)\n",
    "# Set number of topics to 4\n",
    "\n",
    "# Convert the vectorized data to a gensim corpus object\n",
    "corpus_vect = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "id2word = dict((v,k) for k,v in vectorizer.vocabulary_.items())\n",
    "#print(id2word)\n",
    "\n",
    "# Build the lda model\n",
    "lda = LdaModel(corpus_vect, \n",
    "               num_topics=4,\n",
    "               #random_state=54321,\n",
    "               id2word=id2word, \n",
    "               passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.006*\"use\" + 0.004*\"product\" + 0.004*\"work\" + 0.004*\"softwar\" + '\n",
      "  '0.004*\"program\" + 0.003*\"time\" + 0.003*\"great\" + 0.003*\"like\" + '\n",
      "  '0.003*\"game\" + 0.003*\"good\"'),\n",
      " (1,\n",
      "  '0.010*\"watch\" + 0.006*\"great\" + 0.005*\"price\" + 0.004*\"batteri\" + '\n",
      "  '0.004*\"charger\" + 0.004*\"work\" + 0.004*\"love\" + 0.004*\"product\" + '\n",
      "  '0.004*\"bought\" + 0.004*\"amazon\"'),\n",
      " (2,\n",
      "  '0.002*\"dent\" + 0.001*\"snow\" + 0.001*\"la\" + 0.001*\"el\" + 0.001*\"cap\" + '\n",
      "  '0.001*\"tester\" + 0.001*\"en\" + 0.001*\"cat\" + 0.001*\"es\" + 0.001*\"lite\"'),\n",
      " (3,\n",
      "  '0.008*\"watch\" + 0.005*\"use\" + 0.005*\"great\" + 0.004*\"work\" + 0.004*\"air\" + '\n",
      "  '0.004*\"bed\" + 0.004*\"good\" + 0.004*\"like\" + 0.004*\"batteri\" + 0.004*\"fit\"')]\n"
     ]
    }
   ],
   "source": [
    "# Q5.2: Print the topics\n",
    "pprint(lda.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Examine the representative words for each topic. Please create a text box and discuss how well these words describe the 4 product categories, and also tell me which unsupervised method (clustering vs LDA) you think is more effective in identifying the categories in this example and why?**\n",
    "\n",
    "A: The LDA model seems to have performed worse than clustering.  The first two topics map up well to our categories:\n",
    "\n",
    "* Topic 0 maps to Software \n",
    "* Topic 1 maps to Watch  \n",
    "\n",
    "Topic 2 is a bit less coherent.  It might map up to Automotive, as it contains the words \"dent\" and \"snow\".  \n",
    "\n",
    "Topic 3 looks like it maps up with the Watch category again though, duplicating topic 1.\n",
    "\n",
    "I think overall K-means Clustering had more coherent topics, even if it did highlight a topic that wasn't one of our categories (air mattress).  K-means Clustering had 3 of the 4 topics that were more clear at identifying our review categories, so I would pick it over LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6: Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6.1: Vectorize all of the document again, use norm=l2 & drop any words appearing only once\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(norm=\"l2\",\n",
    "                             max_df=0.8, # remove frequent words (>80%)\n",
    "                             stop_words='english', # remove English stopwords stored in Scikit-Learn\n",
    "                             min_df=1 # remove unique words, appearing in just 1 document\n",
    "                            )\n",
    "\n",
    "X = vectorizer.fit_transform(processed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 4000, n_features: 10833\n"
     ]
    }
   ],
   "source": [
    "# Print the number of features extracted by the TFIDF vectorizer\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Number of features: 9726\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.39      0.52       119\n",
      "           2       0.27      0.06      0.10        52\n",
      "           3       0.10      0.03      0.05        61\n",
      "           4       0.26      0.14      0.18       155\n",
      "           5       0.62      0.93      0.74       414\n",
      "\n",
      "   micro avg       0.57      0.57      0.57       801\n",
      "   macro avg       0.40      0.31      0.32       801\n",
      "weighted avg       0.50      0.57      0.50       801\n",
      "\n",
      "Fold 2\n",
      "Number of features: 9078\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.46      0.63      0.53       119\n",
      "           2       0.13      0.06      0.08        52\n",
      "           3       0.17      0.11      0.14        61\n",
      "           4       0.29      0.23      0.25       155\n",
      "           5       0.68      0.74      0.71       414\n",
      "\n",
      "   micro avg       0.53      0.53      0.53       801\n",
      "   macro avg       0.34      0.35      0.34       801\n",
      "weighted avg       0.50      0.53      0.51       801\n",
      "\n",
      "Fold 3\n",
      "Number of features: 9412\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.69      0.63       119\n",
      "           2       0.30      0.13      0.19        52\n",
      "           3       0.15      0.08      0.11        61\n",
      "           4       0.32      0.35      0.34       155\n",
      "           5       0.70      0.74      0.72       414\n",
      "\n",
      "   micro avg       0.57      0.57      0.57       801\n",
      "   macro avg       0.41      0.40      0.39       801\n",
      "weighted avg       0.54      0.57      0.55       801\n",
      "\n",
      "Fold 4\n",
      "Number of features: 10080\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.64      0.58      0.61       119\n",
      "           2       0.30      0.17      0.22        52\n",
      "           3       0.26      0.10      0.14        60\n",
      "           4       0.43      0.27      0.33       154\n",
      "           5       0.65      0.85      0.74       414\n",
      "\n",
      "   micro avg       0.60      0.60      0.60       799\n",
      "   macro avg       0.46      0.40      0.41       799\n",
      "weighted avg       0.56      0.60      0.56       799\n",
      "\n",
      "Fold 5\n",
      "Number of features: 9944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.40      0.48       119\n",
      "           2       0.29      0.10      0.15        51\n",
      "           3       0.38      0.10      0.16        60\n",
      "           4       0.33      0.10      0.16       154\n",
      "           5       0.60      0.92      0.73       414\n",
      "\n",
      "   micro avg       0.57      0.57      0.57       798\n",
      "   macro avg       0.44      0.33      0.33       798\n",
      "weighted avg       0.51      0.57      0.50       798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q6.2: Perform 5-fold cross-validation using SGD classifier\n",
    "# Use norm=l2\n",
    "# Drop any words appearing in only 1 document\n",
    "\n",
    "f1_overall_avg = []\n",
    "\n",
    "skf = model_selection.StratifiedKFold(n_splits=5)\n",
    "fold = 0\n",
    "for train_index, test_index in skf.split(np.array(processed_doc), df[\"score\"]):\n",
    "    fold += 1\n",
    "    print(\"Fold %d\" % fold)\n",
    "    # partition\n",
    "    train_x, test_x = np.array(processed_doc)[train_index], np.array(processed_doc)[test_index]\n",
    "    train_y, test_y = df[\"score\"][train_index], df[\"score\"][test_index]\n",
    "    # vectorize\n",
    "    vectorizer = TfidfVectorizer(norm=\"l2\",\n",
    "                                 max_df=0.8, # remove frequent words (>80%)\n",
    "                                 stop_words='english', # remove English stopwords stored in Scikit-Learn\n",
    "                                 min_df=1 # remove unique words, appearing in just 1 document\n",
    "                                )\n",
    "    X = vectorizer.fit_transform(train_x)\n",
    "    print(\"Number of features: %d\" % len(vectorizer.vocabulary_))\n",
    "    X_test = vectorizer.transform(test_x)\n",
    "    # train model\n",
    "    clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
    "    clf.fit(X, train_y)\n",
    "    # predict\n",
    "    pred = clf.predict(X_test)\n",
    "    # classification results\n",
    "    for line in metrics.classification_report(test_y, pred).split(\"\\n\"):\n",
    "        print(line)\n",
    "    # Save average across fold\n",
    "    f1_overall_avg.append(metrics.f1_score(test_y, pred, labels=None, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1 score across all 5 folds:\n",
      " 0.52\n"
     ]
    }
   ],
   "source": [
    "# Print overall average f1 score across 5 folds\n",
    "f1_score = sum(f1_overall_avg) / float(len(f1_overall_avg))\n",
    "\n",
    "print(\"Average f1 score across all 5 folds:\\n {:.2}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: Satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>helpful</th>\n",
       "      <th>score</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>satisfaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000GAYQL8</td>\n",
       "      <td>0/0</td>\n",
       "      <td>5</td>\n",
       "      <td>GREAT WATCH AND GREAT LOOK. BIG FACE AND 4 DIF...</td>\n",
       "      <td>watch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000IBNPDA</td>\n",
       "      <td>0/0</td>\n",
       "      <td>5</td>\n",
       "      <td>Bought this as a Christmas gift, my boyfriend ...</td>\n",
       "      <td>watch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000J2HA16</td>\n",
       "      <td>0/0</td>\n",
       "      <td>5</td>\n",
       "      <td>I love this watch! Its sporty, without looking...</td>\n",
       "      <td>watch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000BDIQPM</td>\n",
       "      <td>0/0</td>\n",
       "      <td>5</td>\n",
       "      <td>Works great,looks nice,dont have to worry abou...</td>\n",
       "      <td>watch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000GZTH9E</td>\n",
       "      <td>0/3</td>\n",
       "      <td>4</td>\n",
       "      <td>I need to change the watch wrist and I havent ...</td>\n",
       "      <td>watch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B000GB0G7A</td>\n",
       "      <td>0/0</td>\n",
       "      <td>4</td>\n",
       "      <td>There is not much to this gloriously inexpensi...</td>\n",
       "      <td>watch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B0000643Q6</td>\n",
       "      <td>2-Feb</td>\n",
       "      <td>4</td>\n",
       "      <td>Have always loved Movado designs and when I lo...</td>\n",
       "      <td>watch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B000GAYQJK</td>\n",
       "      <td>0/0</td>\n",
       "      <td>5</td>\n",
       "      <td>this watch is cool you can switch the settings...</td>\n",
       "      <td>watch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B0002XV266</td>\n",
       "      <td>Apr-42</td>\n",
       "      <td>1</td>\n",
       "      <td>Materialism involves the importance one attach...</td>\n",
       "      <td>watch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B000E4ARN2</td>\n",
       "      <td>2-Jan</td>\n",
       "      <td>5</td>\n",
       "      <td>I bought this watch just a short time ago and ...</td>\n",
       "      <td>watch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>B00007E7KA</td>\n",
       "      <td>2-Jan</td>\n",
       "      <td>5</td>\n",
       "      <td>This watch is simple to use and the main purpo...</td>\n",
       "      <td>watch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B00020J16M</td>\n",
       "      <td>1-Jan</td>\n",
       "      <td>3</td>\n",
       "      <td>At least this is what I would like to think. I...</td>\n",
       "      <td>watch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>B000936JGC</td>\n",
       "      <td>0/0</td>\n",
       "      <td>4</td>\n",
       "      <td>I have owned this watch for at least two years...</td>\n",
       "      <td>watch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>B000FGEE7E</td>\n",
       "      <td>1-Jan</td>\n",
       "      <td>5</td>\n",
       "      <td>The BEST Watchwinder I have owned. It's on 24/...</td>\n",
       "      <td>watch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>B000GX6SKW</td>\n",
       "      <td>0/0</td>\n",
       "      <td>3</td>\n",
       "      <td>It's a watch! What can I say, it has a date bu...</td>\n",
       "      <td>watch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>B0009P66Z4</td>\n",
       "      <td>0/0</td>\n",
       "      <td>2</td>\n",
       "      <td>I own at least 30 Invictas and love most of th...</td>\n",
       "      <td>watch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>B000FF5DCU</td>\n",
       "      <td>0/0</td>\n",
       "      <td>3</td>\n",
       "      <td>It's got all the bells and whistles - solar po...</td>\n",
       "      <td>watch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>B000E8J5LI</td>\n",
       "      <td>0/0</td>\n",
       "      <td>4</td>\n",
       "      <td>This is a nice watch. It goes with everything ...</td>\n",
       "      <td>watch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>B0000UIVMY</td>\n",
       "      <td>14-Mar</td>\n",
       "      <td>4</td>\n",
       "      <td>This watch sells directly from Timex for $54.9...</td>\n",
       "      <td>watch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>B000HX7F08</td>\n",
       "      <td>0/0</td>\n",
       "      <td>5</td>\n",
       "      <td>I purchased this watch case for my father in l...</td>\n",
       "      <td>watch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pid helpful  score  \\\n",
       "0   B000GAYQL8     0/0      5   \n",
       "1   B000IBNPDA     0/0      5   \n",
       "2   B000J2HA16     0/0      5   \n",
       "3   B000BDIQPM     0/0      5   \n",
       "4   B000GZTH9E     0/3      4   \n",
       "5   B000GB0G7A     0/0      4   \n",
       "6   B0000643Q6   2-Feb      4   \n",
       "7   B000GAYQJK     0/0      5   \n",
       "8   B0002XV266  Apr-42      1   \n",
       "9   B000E4ARN2   2-Jan      5   \n",
       "10  B00007E7KA   2-Jan      5   \n",
       "11  B00020J16M   1-Jan      3   \n",
       "12  B000936JGC     0/0      4   \n",
       "13  B000FGEE7E   1-Jan      5   \n",
       "14  B000GX6SKW     0/0      3   \n",
       "15  B0009P66Z4     0/0      2   \n",
       "16  B000FF5DCU     0/0      3   \n",
       "17  B000E8J5LI     0/0      4   \n",
       "18  B0000UIVMY  14-Mar      4   \n",
       "19  B000HX7F08     0/0      5   \n",
       "\n",
       "                                                 text category  satisfaction  \n",
       "0   GREAT WATCH AND GREAT LOOK. BIG FACE AND 4 DIF...    watch             1  \n",
       "1   Bought this as a Christmas gift, my boyfriend ...    watch             1  \n",
       "2   I love this watch! Its sporty, without looking...    watch             1  \n",
       "3   Works great,looks nice,dont have to worry abou...    watch             1  \n",
       "4   I need to change the watch wrist and I havent ...    watch             1  \n",
       "5   There is not much to this gloriously inexpensi...    watch             1  \n",
       "6   Have always loved Movado designs and when I lo...    watch             1  \n",
       "7   this watch is cool you can switch the settings...    watch             1  \n",
       "8   Materialism involves the importance one attach...    watch             0  \n",
       "9   I bought this watch just a short time ago and ...    watch             1  \n",
       "10  This watch is simple to use and the main purpo...    watch             1  \n",
       "11  At least this is what I would like to think. I...    watch             0  \n",
       "12  I have owned this watch for at least two years...    watch             1  \n",
       "13  The BEST Watchwinder I have owned. It's on 24/...    watch             1  \n",
       "14  It's a watch! What can I say, it has a date bu...    watch             0  \n",
       "15  I own at least 30 Invictas and love most of th...    watch             0  \n",
       "16  It's got all the bells and whistles - solar po...    watch             0  \n",
       "17  This is a nice watch. It goes with everything ...    watch             1  \n",
       "18  This watch sells directly from Timex for $54.9...    watch             1  \n",
       "19  I purchased this watch case for my father in l...    watch             1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7.1: Create a new variable \"satisfaction\", based off score data\n",
    "\n",
    "# Make a copy of the original dataframe\n",
    "df_sat = pd.DataFrame(df)\n",
    "\n",
    "# Add satisfaction column, code based off score values\n",
    "df_sat['satisfaction'] = np.where(df_sat['score']<4, 0, 1)\n",
    "\n",
    "# Ensure we have a new satisfaction column\n",
    "df_sat.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Number of features: 9722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.15      0.26       232\n",
      "           1       0.74      0.99      0.85       569\n",
      "\n",
      "   micro avg       0.75      0.75      0.75       801\n",
      "   macro avg       0.80      0.57      0.55       801\n",
      "weighted avg       0.77      0.75      0.68       801\n",
      "\n",
      "Fold 2\n",
      "Number of features: 9103\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.69      0.67       232\n",
      "           1       0.87      0.85      0.86       569\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       801\n",
      "   macro avg       0.76      0.77      0.77       801\n",
      "weighted avg       0.81      0.80      0.80       801\n",
      "\n",
      "Fold 3\n",
      "Number of features: 9390\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.69      0.69       231\n",
      "           1       0.88      0.88      0.88       569\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       800\n",
      "   macro avg       0.78      0.78      0.78       800\n",
      "weighted avg       0.82      0.82      0.82       800\n",
      "\n",
      "Fold 4\n",
      "Number of features: 10086\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.55      0.65       231\n",
      "           1       0.84      0.94      0.88       568\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       799\n",
      "   macro avg       0.81      0.74      0.77       799\n",
      "weighted avg       0.82      0.83      0.82       799\n",
      "\n",
      "Fold 5\n",
      "Number of features: 9941\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.42      0.54       231\n",
      "           1       0.80      0.95      0.87       568\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       799\n",
      "   macro avg       0.79      0.68      0.70       799\n",
      "weighted avg       0.79      0.80      0.77       799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q7.2: Perform a 5-fold cross validation to predict satisfaction using an SGD classifier\n",
    "\n",
    "f1_overall_avg = []\n",
    "\n",
    "skf = model_selection.StratifiedKFold(n_splits=5)\n",
    "fold = 0\n",
    "for train_index, test_index in skf.split(np.array(processed_doc), df_sat[\"satisfaction\"]):\n",
    "    fold += 1\n",
    "    print(\"Fold %d\" % fold)\n",
    "    # partition\n",
    "    train_x, test_x = np.array(processed_doc)[train_index], np.array(processed_doc)[test_index]\n",
    "    train_y, test_y = df_sat[\"satisfaction\"][train_index], df_sat[\"satisfaction\"][test_index]\n",
    "    # vectorize\n",
    "    vectorizer = TfidfVectorizer(norm=\"l2\",\n",
    "                                 max_df=0.8, # remove frequent words (>80%)\n",
    "                                 stop_words='english', # remove English stopwords stored in Scikit-Learn\n",
    "                                 min_df=1 # remove unique words, appearing in just 1 document\n",
    "                                )\n",
    "    X = vectorizer.fit_transform(train_x)\n",
    "    print(\"Number of features: %d\" % len(vectorizer.vocabulary_))\n",
    "    X_test = vectorizer.transform(test_x)\n",
    "    # train model\n",
    "    clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
    "    clf.fit(X, train_y)\n",
    "    # predict\n",
    "    pred = clf.predict(X_test)\n",
    "    # classification results\n",
    "    for line in metrics.classification_report(test_y, pred).split(\"\\n\"):\n",
    "        print(line)\n",
    "    # Save average across fold\n",
    "    f1_overall_avg.append(metrics.f1_score(test_y, pred, labels=None, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1 score across all 5 folds:\n",
      " 0.78\n"
     ]
    }
   ],
   "source": [
    "# Print overall average f1 score across 5 folds\n",
    "f1_score = sum(f1_overall_avg) / float(len(f1_overall_avg))\n",
    "\n",
    "print(\"Average f1 score across all 5 folds:\\n {:.2}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8: Vectorize again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-faced -1\n",
      "2-faces -1\n",
      "abnormal -1\n",
      "abolish -1\n",
      "abominable -1\n",
      "abominably -1\n"
     ]
    }
   ],
   "source": [
    "# Q8.1: Use the opinion lexicon described in the lecture as the vocabulary\n",
    "\n",
    "# read the lexicon\n",
    "lexicon = dict()\n",
    "\n",
    "# read postive words\n",
    "with open(\"opinion-lexicon-English/negative-words.txt\", \"r\") as in_file:\n",
    "    for line in in_file.readlines():\n",
    "        if not line.startswith(\";\") and line != \"\\n\":\n",
    "            lexicon[line.strip()] = -1\n",
    "\n",
    "# read negative words\n",
    "with open(\"opinion-lexicon-English/positive-words.txt\", \"r\") as in_file:\n",
    "    for line in in_file.readlines():\n",
    "        if not line.startswith(\";\") and line != \"\\n\":\n",
    "            lexicon[line.strip()] = 1\n",
    "\n",
    "# print the top 5 entries\n",
    "for i, (k, v) in enumerate(lexicon.items()):\n",
    "    print(k, v)\n",
    "    if i > 4: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Number of features: 6786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.21      0.31       232\n",
      "           1       0.75      0.95      0.84       569\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       801\n",
      "   macro avg       0.70      0.58      0.58       801\n",
      "weighted avg       0.72      0.74      0.69       801\n",
      "\n",
      "Fold 2\n",
      "Number of features: 6786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.53      0.56       232\n",
      "           1       0.82      0.85      0.84       569\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       801\n",
      "   macro avg       0.71      0.69      0.70       801\n",
      "weighted avg       0.75      0.76      0.76       801\n",
      "\n",
      "Fold 3\n",
      "Number of features: 6786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.45      0.54       231\n",
      "           1       0.80      0.91      0.85       569\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       800\n",
      "   macro avg       0.74      0.68      0.70       800\n",
      "weighted avg       0.76      0.78      0.76       800\n",
      "\n",
      "Fold 4\n",
      "Number of features: 6786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.42      0.52       231\n",
      "           1       0.80      0.91      0.85       568\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       799\n",
      "   macro avg       0.73      0.67      0.68       799\n",
      "weighted avg       0.76      0.77      0.75       799\n",
      "\n",
      "Fold 5\n",
      "Number of features: 6786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.39      0.45       231\n",
      "           1       0.78      0.86      0.82       568\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       799\n",
      "   macro avg       0.65      0.63      0.63       799\n",
      "weighted avg       0.71      0.72      0.71       799\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform a 5-fold cross validation to predict satisfaction using an SGD classifier\n",
    "\n",
    "vocab = lexicon.keys()\n",
    "\n",
    "f1_overall_avg = []\n",
    "\n",
    "skf = model_selection.StratifiedKFold(n_splits=5)\n",
    "fold = 0\n",
    "for train_index, test_index in skf.split(np.array(processed_doc), df_sat[\"satisfaction\"]):\n",
    "    fold += 1\n",
    "    print(\"Fold %d\" % fold)\n",
    "    # partition\n",
    "    train_x, test_x = np.array(processed_doc)[train_index], np.array(processed_doc)[test_index]\n",
    "    train_y, test_y = df_sat[\"satisfaction\"][train_index], df_sat[\"satisfaction\"][test_index]\n",
    "    # vectorize\n",
    "    vectorizer = TfidfVectorizer(norm=\"l2\",\n",
    "                                 max_df=0.8, # remove frequent words (>80%)\n",
    "                                 stop_words='english', # remove English stopwords stored in Scikit-Learn\n",
    "                                 min_df=1, # remove unique words, appearing in just 1 document\n",
    "                                 vocabulary=vocab # use our opinion lexicon\n",
    "                                )\n",
    "    X = vectorizer.fit_transform(train_x)\n",
    "    print(\"Number of features: %d\" % len(vectorizer.vocabulary_))\n",
    "    X_test = vectorizer.transform(test_x)\n",
    "    # train model\n",
    "    clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
    "    clf.fit(X, train_y)\n",
    "    # predict\n",
    "    pred = clf.predict(X_test)\n",
    "    # classification results\n",
    "    for line in metrics.classification_report(test_y, pred).split(\"\\n\"):\n",
    "        print(line)\n",
    "    # Save average across fold\n",
    "    f1_overall_avg.append(metrics.f1_score(test_y, pred, labels=None, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1 score across all 5 folds:\n",
      " 0.73\n"
     ]
    }
   ],
   "source": [
    "# Print overall average f1 score across 5 folds\n",
    "f1_score = sum(f1_overall_avg) / float(len(f1_overall_avg))\n",
    "\n",
    "print(\"Average f1 score across all 5 folds:\\n {:.2}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Tell me if the average F1 score has increased? If so, why?**\n",
    "\n",
    "A: No, the average F1 score has not increased from Q7.  Instead, it went down by about 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10833\n",
      "[1.38860801e-02 3.83311952e-03 3.20978202e-03 ... 8.53055910e-39\n",
      " 5.47672946e-40 2.31403188e-40]\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "vectorizer = TfidfVectorizer(norm=\"l2\",\n",
    "                             max_df=0.8, # remove frequent words (>80%)\n",
    "                             stop_words='english', # remove English stopwords stored in Scikit-Learn\n",
    "                             min_df=1 # remove unique words, appearing in just 1 document\n",
    "                            )\n",
    "\n",
    "X = vectorizer.fit_transform(processed_doc).todense()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(X)\n",
    "print(len(X[0]))\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(svd_solver='randomized',whiten=True).fit(X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "sumofvariance=0.0\n",
    "n_components = 0\n",
    "for item in pca.explained_variance_ratio_:\n",
    "    sumofvariance += item\n",
    "    n_components+=1\n",
    "    if sumofvariance>=0.9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Please create a textbox and use your own words to briefly describe PCA**\n",
    "\n",
    "A: Principle Component analysis is a method where you can use the relationships between existing variables in your data to derive new variables.  From those new variables, you can select the most important ones that help best explain the variance in your data.  Those most important new variables are called the principle components, and since you're usually left with a smaller number of principle components than the original number of variables in your data, using PCA can help reduce the dimensionality of your data to make it simpler to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 2103\n"
     ]
    }
   ],
   "source": [
    "# Print the number of components:\n",
    "\n",
    "print(\"Number of components: {}\".format(n_components))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.45      0.53       231\n",
      "           1       0.80      0.89      0.84       569\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       800\n",
      "   macro avg       0.71      0.67      0.68       800\n",
      "weighted avg       0.75      0.76      0.75       800\n",
      "\n",
      "Fold 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.37      0.45       231\n",
      "           1       0.78      0.89      0.83       569\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       800\n",
      "   macro avg       0.68      0.63      0.64       800\n",
      "weighted avg       0.72      0.74      0.72       800\n",
      "\n",
      "Fold 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.42      0.50       231\n",
      "           1       0.79      0.89      0.84       569\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       800\n",
      "   macro avg       0.70      0.66      0.67       800\n",
      "weighted avg       0.74      0.76      0.74       800\n",
      "\n",
      "Fold 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.36      0.45       231\n",
      "           1       0.78      0.89      0.83       569\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       800\n",
      "   macro avg       0.68      0.63      0.64       800\n",
      "weighted avg       0.72      0.74      0.72       800\n",
      "\n",
      "Fold 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.39      0.47       231\n",
      "           1       0.78      0.89      0.83       569\n",
      "\n",
      "   micro avg       0.75      0.75      0.75       800\n",
      "   macro avg       0.69      0.64      0.65       800\n",
      "weighted avg       0.73      0.75      0.73       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform a 5-fold cross validation to predict satisfaction using an SGD classifier.\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',whiten=True).fit(X)\n",
    "X_train_pca = pca.transform(X)\n",
    "\n",
    "f1_overall_avg = []\n",
    "\n",
    "skf = model_selection.StratifiedKFold(n_splits=5)\n",
    "fold = 0\n",
    "for fold in range(5):\n",
    "    fold += 1\n",
    "    print(\"Fold %d\" % fold)\n",
    "    # partition\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X_train_pca, \n",
    "                                                        df_sat[\"satisfaction\"], \n",
    "                                                        test_size=0.2, \n",
    "                                                        stratify=df_sat[\"satisfaction\"])\n",
    "    # train model\n",
    "    clf = SGDClassifier()\n",
    "    clf.fit(train_x, train_y)\n",
    "    # predict\n",
    "    pred_y = clf.predict(test_x)\n",
    "    # classification results\n",
    "    for line in metrics.classification_report(test_y, pred_y).split(\"\\n\"):\n",
    "        print(line)\n",
    "    # Save average across fold\n",
    "    f1_overall_avg.append(metrics.f1_score(test_y, pred_y, labels=None, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1 score across all 5 folds:\n",
      " 0.73\n"
     ]
    }
   ],
   "source": [
    "# Print overall average f1 score across 5 folds\n",
    "f1_score = sum(f1_overall_avg) / float(len(f1_overall_avg))\n",
    "\n",
    "print(\"Average f1 score across all 5 folds:\\n {:.2}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Tell me if the average F1 score has increased? If so, why?**\n",
    "\n",
    "A: No, the average F1 score has not increased from Q7, and the score remains the same as Q8.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
